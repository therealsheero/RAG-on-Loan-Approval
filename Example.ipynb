{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mbLhJ4YrrkU"
      },
      "source": [
        "# 🚀 **Building a Simple RAG System**  \n",
        "\n",
        "Retrieval-Augmented Generation (**RAG**) enhances **LLMs** by retrieving relevant data before generating responses. Follow these steps to implement a basic **RAG pipeline**:  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **1. Gather Documents**  \n",
        "📌 Collect structured/unstructured data such as:  \n",
        "- PDFs 📄  \n",
        "- Excel Sheets 📊  \n",
        "- Word Documents 📝  \n",
        "- Web Articles 🌐  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **2. Load the Documents**  \n",
        "Use libraries like:  \n",
        "```python\n",
        "from langchain.document_loaders import PyPDFLoader, CSVLoader, UnstructuredWordDocumentLoader\n",
        "```\n",
        "- PDFs → `PyPDFLoader()`  \n",
        "- Excel → `CSVLoader()`  \n",
        "- Word → `UnstructuredWordDocumentLoader()`  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **3. Split the Text**  \n",
        "**Chunk large documents** for efficient retrieval:  \n",
        "- **Option 1:** Split by **pages**  \n",
        "- **Option 2:** Split by **logical flows (sections/paragraphs)**  \n",
        "\n",
        "```python\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **4. Chunk the Split Data**  \n",
        "Breaking down text into **manageable chunks** improves retrieval accuracy.  \n",
        "\n",
        "✅ **Chunking Example:**  \n",
        "```\n",
        "Chunk 1: \"Introduction to RAG...\"\n",
        "Chunk 2: \"RAG uses embeddings to enhance LLMs...\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **5. Create Embeddings of the Chunked Data**  \n",
        "Convert text chunks into **vector embeddings** using models like:  \n",
        "- `text-embedding-ada-002` (OpenAI)  \n",
        "- `all-MiniLM-L6-v2` (Sentence Transformers)  \n",
        "\n",
        "```python\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "vector_data = embeddings.embed_documents(chunks)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **6. Instantiate the Vector Store**  \n",
        "Choose a **Vector Database**:  \n",
        "✅ **Local**: FAISS, ChromaDB  \n",
        "✅ **Cloud**: Pinecone, Weaviate, MongoDBAtlasVectorSearch  \n",
        "\n",
        "Example using **FAISS**:  \n",
        "```python\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **7. Add Chunk Embeddings to the Vector Store**  \n",
        "Store **chunked embeddings** for fast retrieval:  \n",
        "\n",
        "```python\n",
        "vectorstore.add_documents(chunks)\n",
        "```\n",
        "\n",
        "Now, **queries** can retrieve the **most relevant** document chunks!\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **8. Initialize the Chat Model**  \n",
        "Use **local** or **cloud-based** LLMs:  \n",
        "✅ **Local:** Ollama, LM Studio  \n",
        "✅ **Cloud:** OpenAI, AzureAI, AWS  \n",
        "\n",
        "```python\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **9. Set Instructions for the Model**  \n",
        "Define **chatting templates** or **string prompts** to guide the model.  \n",
        "\n",
        "✅ Example Prompt:  \n",
        "```\n",
        "Use the retrieved context to answer the user's query concisely.\n",
        "```\n",
        "\n",
        "```python\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"Answer the question based on context:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **10. Create the Retrieval Chain**  \n",
        "**Chains** sequence multiple **LLM calls, tools, and preprocessing steps**.  \n",
        "Use **LCEL (LangChain Expression Language)** to build a chain.\n",
        "\n",
        "```python\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())  ### Example Chain (Now deprecated)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Final Architecture**  \n",
        "```\n",
        "1️⃣ User Query  ➝  2️⃣ Query Embedding  ➝  3️⃣ Vector Search  \n",
        "  ➝  4️⃣ Retrieve Top-K Chunks  ➝  5️⃣ LLM Generation  ➝  6️⃣ Final Answer  \n",
        "```\n",
        "\n",
        "🔹 Now, your **RAG system** is ready to **retrieve & generate** answers efficiently! 🚀  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPTr2Ggarrkg"
      },
      "source": [
        "### STEP-1\n",
        "Gather documents\n",
        "We will be using text type documents and pdf type documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmXrPYMGrrki",
        "outputId": "53c83249-6cc9-4422-dcc5-b24597028991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1.8/2.5 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/300.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain_community pypdf  # You can checkout complete list of document loaders available in langchain at https://python.langchain.com/docs/integrations/document_loaders/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBjmNtu7rrkl"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain (Install lanchain)\n",
        "\n",
        "from langchain.document_loaders import TextLoader # Simple TextLoader\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader  # One of the many PDF loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5AMwTKUrrkn",
        "outputId": "6b534bc7-a171-4fdf-b6c3-0a3f2d1be06d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (5.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install chardet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06ejGKLvrrkp"
      },
      "source": [
        "### STEP-2\n",
        "Load relevant documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD83p3m8rrkq",
        "outputId": "b443540c-592e-4971-a03e-96cd5a0ba6cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "137\n",
            "{'producer': 'calibre (0.7.50) [http://calibre-ebook.com]',\n",
            " 'creator': 'calibre (0.7.50) [http://calibre-ebook.com]',\n",
            " 'creationdate': '2022-09-20T04:19:35+00:00',\n",
            " 'author': 'Franklin W. Dixon',\n",
            " 'keywords': 'Hardy Boys (Fictitious characters), Detective and mystery '\n",
            "             'stories, Brothers, Teenage boy detectives, Mystery & Detective, '\n",
            "             'Juvenile Fiction, Mysteries & Detective Stories, General, '\n",
            "             \"Children's stories; American, Mystery fiction, Fiction, \"\n",
            "             'Detective and mystery stories; American, Mystery and detective '\n",
            "             'stories',\n",
            " 'moddate': '2022-09-20T04:19:36+00:00',\n",
            " 'title': 'The Tower Treasure',\n",
            " 'source': 'book1.pdf',\n",
            " 'total_pages': 137,\n",
            " 'page': 0,\n",
            " 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "loader = PyPDFLoader(\n",
        "    \"book1.pdf\",\n",
        "    mode=\"page\",                                            # You can use mode =\"simple\" as well for whole doc as single unit\n",
        ")\n",
        "docs = loader.load()\n",
        "print(len(docs))\n",
        "pprint.pp(docs[0].metadata)\n",
        "\n",
        "\n",
        "# loader = TextLoader('odyssey.txt',autodetect_encoding=True) # In case text document has unknown encodings use autodetect\n",
        "# docs = loader.load()\n",
        "# docs = loader.load()\n",
        "# print(len(docs))\n",
        "# pprint.pp(docs[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqqPftUMvyKs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbzZS1n5rrks"
      },
      "source": [
        "### STEP-3 & 4\n",
        "Splitting the text into chunks\n",
        "\n",
        "There are various methods to split the text in langchain\n",
        "\n",
        "1. Character splitter: Splits based on characters\n",
        "2. Sentence splitter: Splits based on sentences\n",
        "3. Token splitter: Splits based on tokens\n",
        "4. Recursive splitter: Mix of sentences, paragraphs (Most used)\n",
        "5. You can create your own as well!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR6D9tVyrrkt"
      },
      "outputs": [],
      "source": [
        "# import various splitters\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    SentenceTransformersTokenTextSplitter,\n",
        "    TextSplitter,\n",
        "    TokenTextSplitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P5qm4mErrkv",
        "outputId": "a4da4d9a-9a52-4250-c1d8-1c51f7507bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Using Recursive Character-based Splitting ---\n"
          ]
        }
      ],
      "source": [
        "# # Useful for consistent chunk sizes regardless of content structure.\n",
        "# print(\"\\n--- Using Character-based Splitting ---\")\n",
        "# char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "# char_docs = char_splitter.split_documents(docs)\n",
        "\n",
        "# # Splits text into chunks based on sentences, ensuring chunks end at sentence boundaries.\n",
        "# # Ideal for maintaining semantic coherence within chunks.\n",
        "# print(\"\\n--- Using Sentence-based Splitting ---\")\n",
        "# sent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\n",
        "# sent_docs = sent_splitter.split_documents(docs)\n",
        "\n",
        "# # Splits text into chunks based on tokens (words or subwords), using tokenizers like GPT-2.\n",
        "# # Useful for transformer models with strict token limits.\n",
        "# print(\"\\n--- Using Token-based Splitting ---\")\n",
        "# token_splitter = TokenTextSplitter(chunk_overlap=0, chunk_size=512)\n",
        "# token_docs = token_splitter.split_documents(docs)\n",
        "\n",
        "# Attempts to split text at natural boundaries (sentences, paragraphs) within character limit.\n",
        "# Balances between maintaining coherence and adhering to character limits.\n",
        "print(\"\\n--- Using Recursive Character-based Splitting ---\")\n",
        "rec_char_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512, chunk_overlap=0)\n",
        "rec_char_docs = rec_char_splitter.split_documents(docs)\n",
        "\n",
        "# Allows creating custom splitting logic based on specific requirements.\n",
        "# Useful for documents with unique structure that standard splitters can't handle.\n",
        "# print(\"\\n--- Using Custom Splitting ---\")\n",
        "\n",
        "\n",
        "# class CustomTextSplitter(TextSplitter):\n",
        "#     def split_text(self, text):\n",
        "#         # Custom logic for splitting text\n",
        "#         return text.split(\"\\n\\n\")  # Example: split by paragraphs\n",
        "\n",
        "\n",
        "# custom_splitter = CustomTextSplitter()\n",
        "# custom_docs = custom_splitter.split_documents(docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhgUphITrrkz"
      },
      "source": [
        "### STEP-5\n",
        "\n",
        "Create embeddings of the chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoLoHHs1yTUk",
        "outputId": "c2468fcd-2752-41d3-8bf0-5db1ecb55335"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-huggingface sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW-yLrUfyXVu"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "# embeddings = OllamaEmbeddings(\n",
        "#     model=\"nomic-embed-text\",                    # nomic-embed-text is a large context length text encoder that surpasses OpenAI text-embedding-ada-002 and text-embedding-3-small performance on short and long context tasks.\n",
        "# )\n",
        "\n",
        "# import getpass\n",
        "# import os\n",
        "\n",
        "# if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "#   os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
        "\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR3UsakBrrk1"
      },
      "source": [
        "### STEP-6\n",
        "\n",
        "Instantiate the vectorDB of your choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcRbSafcrrk1"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# vector_store = Chroma(embedding_function=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcRxnm9mrrk2"
      },
      "source": [
        "### STEP-7\n",
        "\n",
        "Add chunk embeddings to VectorDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpbYJXu7rrk4",
        "outputId": "51d7bf8e-47c8-4d56-91af-b344bcba24be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Creating vector store vector_store ---\n",
            "--- Finished creating vector store <langchain_chroma.vectorstores.Chroma object at 0x0000013FF49D8B60> ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "db_dir = os.path.join(current_dir, \"db3\")\n",
        "\n",
        "# Create function which stops from repititve addition of data into DB multiple times\n",
        "def create_vector_store(docs, store_name):\n",
        "    persistent_directory = os.path.join(db_dir, store_name)  # ChromaDB storage location\n",
        "    if not os.path.exists(persistent_directory):\n",
        "        print(f\"\\n--- Creating vector store {store_name} ---\")\n",
        "        store_name = Chroma.from_documents(\n",
        "            docs, embeddings, persist_directory=persistent_directory\n",
        "        )\n",
        "        print(f\"--- Finished creating vector store {store_name} ---\")\n",
        "    else:\n",
        "        print(\n",
        "            f\"Vector store {store_name} already exists. No need to initialize.\")\n",
        "\n",
        "create_vector_store(docs,'vector_store')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-WaPDXmrrk5",
        "outputId": "137eeb2b-f7ab-467e-d248-74c9e845ef41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Querying the Vector Store vector_store ---\n",
            "\n",
            "--- Relevant Documents for vector_store ---\n",
            "Document 1:\n",
            "Of these men, the Hardys took the reports on the ones who were thin and of\n",
            "medium height.\n",
            "Next came a check by telephone on the whereabouts of these people. All\n",
            "could be accounted for as working some distance from Bayport at the time\n",
            "of the thefts, with one exception.\n",
            "“I’ll bet he’s our man!” Frank exclaimed. “But where is he now?”\n",
            "\n",
            "Source: book1.pdf\n",
            "\n",
            "Document 2:\n",
            "Hardy. Hi, chums!” he said cheerily. “Sorry to be late. My dad had a lot of\n",
            "phoning to do before he left. I was afraid if I’d tried to walk here, I\n",
            "wouldn’t have arrived until tomorrow.”\n",
            "At this point Mr. Hardy spoke up. “As I said before, I think you boys should\n",
            "work in twos. There are only three of you to take care of half the territory.”\n",
            "The detective suddenly grinned boyishly. “How about me teaming up with\n",
            "one of you?”\n",
            "Frank and Joe looked at their dad in delight. “You mean it?” Frank cried\n",
            "out. “I’ll choose you as my partner right now.”\n",
            "“I have a further suggestion,” the detective said. “It’s not going to take you\n",
            "fellows more than three hours to cover the area you’ve laid out. And there’s\n",
            "an additional section I think you might look into.”\n",
            "“What’s that?” Joe inquired.\n",
            "“Willow Grove. That’s a park area, but there’s also a lot of tangled\n",
            "woodland to one side of it. Good place to hide a stolen car.”\n",
            "Mr. Hardy suggested that the boys meet for a picnic lunch at Willow Grove\n",
            "and later do some sleuthing in the vicinity. “That is, provided you haven’t\n",
            "found Chet’s jalopy by that time.”\n",
            "Mrs. Hardy spoke up. “I’ll fix a nice lunch for all of you,” she offered.\n",
            "“That sure would be swell,” Chet said hastily. “You make grand picnic\n",
            "lunches, Mrs. Hardy.”\n",
            "Frank and Joe liked the plan, and it was decided that the boys would have\n",
            "the picnic whether or not they had found the jalopy by one o’clock. Mrs.\n",
            "Hardy said she would relay the news to the other boys when they phoned in.\n",
            "Chet and Joe set off on the Hardy boys’ motorcycles, taking the twelve-to-\n",
            "three segment on the map. Then Mr. Hardy and Frank drove off for the\n",
            "three-to-six area.\n",
            "\n",
            "Source: book1.pdf\n",
            "\n",
            "Document 3:\n",
            "At the next crossing they found old Mike Hal-ley, the flagman there, busy at\n",
            "his job. His bright blue eyes searched their faces for a moment, then he\n",
            "amazed them by saying, “You’re Frank and Joe Hardy, sons of the famous\n",
            "detective Fenton Hardy.”\n",
            "“You know us?” Frank asked. “I must confess I don’t recall having met you\n",
            "before.”\n",
            "“And you ain’t,” the man responded. “But I make it a rule to memorize\n",
            "every face I see in the newspapers. Never know when there’s goin’ to be an\n",
            "accident and I might be called on to identify some people.”\n",
            "The boys gulped at this gruesome thought, then Frank asked Halley if he\n",
            "remembered a railroad man named Red Jackley.\n",
            "“I recollect a man named Jackley, but he wasn’t never called Red when I\n",
            "knew him. I reckon he’s the same fellow, though. You mean the one that I\n",
            "read went to jail?”\n",
            "“That’s the man!”\n",
            "“He out of the pen yet?” Mike Halley questioned.\n",
            "“He died,” Joe replied. “Our dad is working on a case that has some\n",
            "connection with Jackley and we’re just trying to find out something about\n",
            "him.” “Then what you want to do,” said the flagman, “is go down to the\n",
            "Bayport and Coast Line Railroad. That’s where Jackley used to work. He\n",
            "was around the station at Cherryville. That ain’t so far from here.” He\n",
            "pointed in a northerly direction. “Thanks a million,” said Frank. “You’ve\n",
            "helped us a lot.”\n",
            "The brothers set off on their motorcycles for Cherryville. When they came\n",
            "to the small town, a policeman directed them to the railroad station, which\n",
            "was about a half mile out of town. The station stood in a depression below a\n",
            "new highway, and was reached by a curving road which ran parallel to the\n",
            "tracks for several hundred feet.\n",
            "\n",
            "Source: book1.pdf\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(id='416a5ab7-c443-4cbd-8e4c-71e78fc73e2d', metadata={'author': 'Franklin W. Dixon', 'creationdate': '2022-09-20T04:19:35+00:00', 'creator': 'calibre (0.7.50) [http://calibre-ebook.com]', 'keywords': \"Hardy Boys (Fictitious characters), Detective and mystery stories, Brothers, Teenage boy detectives, Mystery & Detective, Juvenile Fiction, Mysteries & Detective Stories, General, Children's stories; American, Mystery fiction, Fiction, Detective and mystery stories; American, Mystery and detective stories\", 'moddate': '2022-09-20T04:19:36+00:00', 'page': 71, 'page_label': '72', 'producer': 'calibre (0.7.50) [http://calibre-ebook.com]', 'source': 'book1.pdf', 'title': 'The Tower Treasure', 'total_pages': 137}, page_content='Of these men, the Hardys took the reports on the ones who were thin and of\\nmedium height.\\nNext came a check by telephone on the whereabouts of these people. All\\ncould be accounted for as working some distance from Bayport at the time\\nof the thefts, with one exception.\\n“I’ll bet he’s our man!” Frank exclaimed. “But where is he now?”'),\n",
              " Document(id='5328223c-4c69-4355-a845-660392eff1bd', metadata={'author': 'Franklin W. Dixon', 'creationdate': '2022-09-20T04:19:35+00:00', 'creator': 'calibre (0.7.50) [http://calibre-ebook.com]', 'keywords': \"Hardy Boys (Fictitious characters), Detective and mystery stories, Brothers, Teenage boy detectives, Mystery & Detective, Juvenile Fiction, Mysteries & Detective Stories, General, Children's stories; American, Mystery fiction, Fiction, Detective and mystery stories; American, Mystery and detective stories\", 'moddate': '2022-09-20T04:19:36+00:00', 'page': 36, 'page_label': '37', 'producer': 'calibre (0.7.50) [http://calibre-ebook.com]', 'source': 'book1.pdf', 'title': 'The Tower Treasure', 'total_pages': 137}, page_content='Hardy. Hi, chums!” he said cheerily. “Sorry to be late. My dad had a lot of\\nphoning to do before he left. I was afraid if I’d tried to walk here, I\\nwouldn’t have arrived until tomorrow.”\\nAt this point Mr. Hardy spoke up. “As I said before, I think you boys should\\nwork in twos. There are only three of you to take care of half the territory.”\\nThe detective suddenly grinned boyishly. “How about me teaming up with\\none of you?”\\nFrank and Joe looked at their dad in delight. “You mean it?” Frank cried\\nout. “I’ll choose you as my partner right now.”\\n“I have a further suggestion,” the detective said. “It’s not going to take you\\nfellows more than three hours to cover the area you’ve laid out. And there’s\\nan additional section I think you might look into.”\\n“What’s that?” Joe inquired.\\n“Willow Grove. That’s a park area, but there’s also a lot of tangled\\nwoodland to one side of it. Good place to hide a stolen car.”\\nMr. Hardy suggested that the boys meet for a picnic lunch at Willow Grove\\nand later do some sleuthing in the vicinity. “That is, provided you haven’t\\nfound Chet’s jalopy by that time.”\\nMrs. Hardy spoke up. “I’ll fix a nice lunch for all of you,” she offered.\\n“That sure would be swell,” Chet said hastily. “You make grand picnic\\nlunches, Mrs. Hardy.”\\nFrank and Joe liked the plan, and it was decided that the boys would have\\nthe picnic whether or not they had found the jalopy by one o’clock. Mrs.\\nHardy said she would relay the news to the other boys when they phoned in.\\nChet and Joe set off on the Hardy boys’ motorcycles, taking the twelve-to-\\nthree segment on the map. Then Mr. Hardy and Frank drove off for the\\nthree-to-six area.'),\n",
              " Document(id='ac682334-1fde-4ae9-a605-af31dddfa294', metadata={'author': 'Franklin W. Dixon', 'creationdate': '2022-09-20T04:19:35+00:00', 'creator': 'calibre (0.7.50) [http://calibre-ebook.com]', 'keywords': \"Hardy Boys (Fictitious characters), Detective and mystery stories, Brothers, Teenage boy detectives, Mystery & Detective, Juvenile Fiction, Mysteries & Detective Stories, General, Children's stories; American, Mystery fiction, Fiction, Detective and mystery stories; American, Mystery and detective stories\", 'moddate': '2022-09-20T04:19:36+00:00', 'page': 119, 'page_label': '120', 'producer': 'calibre (0.7.50) [http://calibre-ebook.com]', 'source': 'book1.pdf', 'title': 'The Tower Treasure', 'total_pages': 137}, page_content='At the next crossing they found old Mike Hal-ley, the flagman there, busy at\\nhis job. His bright blue eyes searched their faces for a moment, then he\\namazed them by saying, “You’re Frank and Joe Hardy, sons of the famous\\ndetective Fenton Hardy.”\\n“You know us?” Frank asked. “I must confess I don’t recall having met you\\nbefore.”\\n“And you ain’t,” the man responded. “But I make it a rule to memorize\\nevery face I see in the newspapers. Never know when there’s goin’ to be an\\naccident and I might be called on to identify some people.”\\nThe boys gulped at this gruesome thought, then Frank asked Halley if he\\nremembered a railroad man named Red Jackley.\\n“I recollect a man named Jackley, but he wasn’t never called Red when I\\nknew him. I reckon he’s the same fellow, though. You mean the one that I\\nread went to jail?”\\n“That’s the man!”\\n“He out of the pen yet?” Mike Halley questioned.\\n“He died,” Joe replied. “Our dad is working on a case that has some\\nconnection with Jackley and we’re just trying to find out something about\\nhim.” “Then what you want to do,” said the flagman, “is go down to the\\nBayport and Coast Line Railroad. That’s where Jackley used to work. He\\nwas around the station at Cherryville. That ain’t so far from here.” He\\npointed in a northerly direction. “Thanks a million,” said Frank. “You’ve\\nhelped us a lot.”\\nThe brothers set off on their motorcycles for Cherryville. When they came\\nto the small town, a policeman directed them to the railroad station, which\\nwas about a half mile out of town. The station stood in a depression below a\\nnew highway, and was reached by a curving road which ran parallel to the\\ntracks for several hundred feet.')]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def query_vector_store(store_name, query, embedding_function):\n",
        "    persistent_directory = os.path.join(db_dir, store_name)\n",
        "    if os.path.exists(persistent_directory):\n",
        "        print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
        "        db = Chroma(\n",
        "            persist_directory=persistent_directory,\n",
        "            embedding_function=embedding_function,\n",
        "        )\n",
        "        retriever = db.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": 3},\n",
        "        )\n",
        "        relevant_docs = retriever.invoke(query)\n",
        "        # Display the relevant results with metadata\n",
        "        print(f\"\\n--- Relevant Documents for {store_name} ---\")\n",
        "        for i, doc in enumerate(relevant_docs, 1):\n",
        "            print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "            if doc.metadata:\n",
        "                print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
        "        return relevant_docs\n",
        "    else:\n",
        "        print(f\"Vector store {store_name} does not exist.\")\n",
        "\n",
        "query = \"Who is frank hardy ?\"\n",
        "query_vector_store(\"vector_store\", query, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3dNYbEZrrk7"
      },
      "source": [
        "### Verification step\n",
        "\n",
        "You can manually check whether the data is correctly added to DB my manually querying into DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbd5KI2drrk8"
      },
      "outputs": [],
      "source": [
        "query = \"Who is frank hardy ?\"\n",
        "# Retrieve relevant documents based on the query\n",
        "\n",
        "relevant_docs = query_vector_store(\"vector_store\", query, embeddings)\n",
        "# print(relevant_docs)\n",
        "# Display the relevant results with metadata\n",
        "print(\"\\n--- Relevant Documents ---\")\n",
        "for i, doc in enumerate(relevant_docs, 1):\n",
        "    # print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "    if doc.metadata:\n",
        "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS7q0S8frrk9"
      },
      "source": [
        "### STEP-8\n",
        "\n",
        "Initialize the chat model of your choice OpenAI or Ollama or HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrcbDKRTrrk9",
        "outputId": "a6cb3e62-9e1d-4290-c715-a218be4b9d32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='**Fibonacci Series in Python**\\n=====================================\\n\\nHere is a simple Python function to generate the Fibonacci series up to the 6th term:\\n\\n```python\\ndef fibonacci(n):\\n    \"\"\"\\n    Generate the Fibonacci series up to the nth term.\\n\\n    Args:\\n        n (int): The number of terms in the series.\\n\\n    Returns:\\n        list: A list of integers representing the Fibonacci series.\\n    \"\"\"\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    elif n == 2:\\n        return [0, 1]\\n\\n    fib_series = [0, 1]\\n    while len(fib_series) < n:\\n        fib_series.append(fib_series[-1] + fib_series[-2])\\n\\n    return fib_series\\n\\n# Print the Fibonacci series up to the 6th term\\nprint(\"Fibonacci Series:\")\\nfor i in range(1, 7):\\n    print(f\"F({i}) = {fibonacci(i)[i-1]}\")\\n```\\n\\n**Output:**\\n```\\nFibonacci Series:\\nF(1) = 0\\nF(2) = 1\\nF(3) = 1\\nF(4) = 2\\nF(5) = 3\\nF(6) = 5\\n```\\n\\nThis code defines a function `fibonacci(n)` that generates the Fibonacci series up to the nth term. It uses a simple iterative approach, starting with the base cases for n=1 and n=2, and then appending each subsequent term to the list using the recursive formula `F(n) = F(n-1) + F(n-2)`. The function returns a list of integers representing the Fibonacci series.\\n\\nThe main part of the code prints the Fibonacci series up to the 6th term by calling the `fibonacci(i)` function for each term from 1 to 6, and then printing the corresponding value in the series.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-02-15T04:59:56.1915126Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2487472100, 'load_duration': 10407200, 'prompt_eval_count': 54, 'prompt_eval_duration': 103000000, 'eval_count': 403, 'eval_duration': 2145000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-5365b076-4839-4642-a39d-c52f50d29132-0', usage_metadata={'input_tokens': 54, 'output_tokens': 403, 'total_tokens': 457})"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# messages = [\n",
        "#     SystemMessage(content=\"Solve the following math problems\"),\n",
        "#     HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "# ]\n",
        "\n",
        "# model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
        "\n",
        "# model.invoke(\"Hello, world!\")\n",
        "# # Create a ChatOpenAI model\n",
        "# model = init_chat_modelodel=\"gpt-4o\")\n",
        "\n",
        "# # Invoke the model with messages\n",
        "# result = model.invoke(messages)\n",
        "# print(f\"Answer from OpenAI: {result.content}\")\n",
        "\n",
        "\n",
        "# # ---- Anthropic Chat Model Example ----\n",
        "\n",
        "# # Create a Anthropic model\n",
        "# # Anthropic models: https://docs.anthropic.com/en/docs/models-overview\n",
        "# model = init_chat_modeldel=(\"claude-3-opus-20240229\")\n",
        "\n",
        "# result = model.invoke(messages)\n",
        "# print(f\"Answer from Anthropic: {result.content}\")\n",
        "\n",
        "\n",
        "# # ---- Google Chat Model Example ----\n",
        "\n",
        "# # https://console.cloud.google.com/gen-app-builder/engines\n",
        "# # https://ai.google.dev/gemini-api/docs/models/gemini\n",
        "# model = init_chat_modelmodel=(\"gemini-1.5-flash\")\n",
        "\n",
        "# result = model.invoke(messages)\n",
        "# print(f\"Answer from Google: {result.content}\")\n",
        "%pip install --upgrade --quiet  langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.2\",\n",
        "    temperature=0,\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that writes professional code. Write good quality code.\",\n",
        "    ),\n",
        "    (\"human\", \"write python code to print fibonaci series til 6th \"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-iS0sUHrrk-"
      },
      "source": [
        "### STEP-9\n",
        "\n",
        "Set instructions for model\n",
        "1. Set prompt for the model\n",
        "2. Combine the retrieval output and the user query (Chaining)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKWcM8j3rrk_",
        "outputId": "f581487b-5045-480d-b493-ba82b241d3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided documents, I can tell you that Frank Hardy is one of the main characters mentioned. He is a young man who works with his brother Joe and their father, Fenton Hardy, to solve mysteries and crimes.\n"
          ]
        }
      ],
      "source": [
        "# from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessage\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_ollama import ChatOllama\n",
        "query = \"Who is frank hardy?\"\n",
        "combined_input = (\n",
        "    \"Here are some documents that might help answer the question: \"\n",
        "    + query\n",
        "    + \"\\n\\nRelevant Documents:\\n\"\n",
        "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    + \"\\n\\nPlease provide an answer based only on the provided documents. If the answer is not found in the documents, respond with 'I'm not sure'.\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=combined_input),\n",
        "]\n",
        "\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.2\",\n",
        "    temperature=0,\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "\n",
        "# Invoke the model with the combined input\n",
        "result = llm.invoke(messages)\n",
        "# Create the ChatPromptTemplate with messages\n",
        "\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB42L2wyrrk_"
      },
      "source": [
        "### STEP-10\n",
        "\n",
        "We will invoke use invoke function with retrieved_doc as context and user query as question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyzEmlEcrrlA",
        "outputId": "9fc376ec-410f-43ab-ea05-37f9f73a14f8"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'RunnableSequence' object has no attribute 'format_messages'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m message1 \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m(context \u001b[38;5;241m=\u001b[39m {relevant_docs},user_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the name of the book ?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m chain\u001b[38;5;241m.\u001b[39minvoke(message1)\n",
            "File \u001b[1;32mc:\\GenAI 2025\\rag\\Lib\\site-packages\\pydantic\\main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'RunnableSequence' object has no attribute 'format_messages'"
          ]
        }
      ],
      "source": [
        "message1 = chain.format_messages(context = {relevant_docs},user_input=\"What is the name of the book ?\")\n",
        "chain.invoke(message1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgxhf7o0rrlB"
      },
      "source": [
        "### Using community made end-to-end chains combined with retrievars\n",
        "We can optionally use prebuilt chains as well to avoid the fuss like RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfzfjdu1rrlC",
        "outputId": "649d3bcd-c970-4a2e-8d5c-75a211225ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Querying the Vector Store  ---\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "persistent_directory = os.path.join(db_dir, 'vector_store')\n",
        "\n",
        "\n",
        "if os.path.exists(persistent_directory):\n",
        "    print(f\"\\n--- Querying the Vector Store  ---\")\n",
        "    db = Chroma(\n",
        "        persist_directory=persistent_directory,\n",
        "        embedding_function=embeddings,\n",
        "    )\n",
        "    retriever = db.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 3},\n",
        "    )\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "    You are a helpful AI assistant that answers questions based on the provided PDF document.\n",
        "    Use only the context provided to answer the question. If you don't know the answer or\n",
        "    can't find it in the context, say so.\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer: Let me help you with that based on the PDF content.\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # 6. Create and return the QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,  # in this we have added the retriever in the chain itself instead of querying it manually first\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": PROMPT}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voEz2Qe1rrlD",
        "outputId": "514cf2f5-8868-453a-fcb6-d6045a98541d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frank Hardy is one of the main characters in the story. He is the son of Fenton Hardy, a famous detective, and is also a young detective himself who works with his brother Joe to solve cases.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "system_prompt = (\n",
        "    \"Use the given context to answer the question. \"\n",
        "    \"If you don't know the answer, say you don't know. \"\n",
        "    \"Use three sentence maximum and keep the answer concise. \"\n",
        "    \"Context: {context}\"\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "result = chain.invoke({\"input\": query})\n",
        "\n",
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
      "state": {},
      "version_major": 2,
      "version_minor": 0

      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
