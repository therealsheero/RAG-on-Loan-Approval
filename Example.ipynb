{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mbLhJ4YrrkU"
      },
      "source": [
        "# ğŸš€ **Building a Simple RAG System**  \n",
        "\n",
        "Retrieval-Augmented Generation (**RAG**) enhances **LLMs** by retrieving relevant data before generating responses. Follow these steps to implement a basic **RAG pipeline**:  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ **1. Gather Documents**  \n",
        "ğŸ“Œ Collect structured/unstructured data such as:  \n",
        "- PDFs ğŸ“„  \n",
        "- Excel Sheets ğŸ“Š  \n",
        "- Word Documents ğŸ“  \n",
        "- Web Articles ğŸŒ  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ **2. Load the Documents**  \n",
        "Use libraries like:  \n",
        "```python\n",
        "from langchain.document_loaders import PyPDFLoader, CSVLoader, UnstructuredWordDocumentLoader\n",
        "```\n",
        "- PDFs â†’ `PyPDFLoader()`  \n",
        "- Excel â†’ `CSVLoader()`  \n",
        "- Word â†’ `UnstructuredWordDocumentLoader()`  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ **3. Split the Text**  \n",
        "**Chunk large documents** for efficient retrieval:  \n",
        "- **Option 1:** Split by **pages**  \n",
        "- **Option 2:** Split by **logical flows (sections/paragraphs)**  \n",
        "\n",
        "```python\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ **4. Chunk the Split Data**  \n",
        "Breaking down text into **manageable chunks** improves retrieval accuracy.  \n",
        "\n",
        "âœ… **Chunking Example:**  \n",
        "```\n",
        "Chunk 1: \"Introduction to RAG...\"\n",
        "Chunk 2: \"RAG uses embeddings to enhance LLMs...\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ **5. Create Embeddings of the Chunked Data**  \n",
        "Convert text chunks into **vector embeddings** using models like:  \n",
        "- `text-embedding-ada-002` (OpenAI)  \n",
        "- `all-MiniLM-L6-v2` (Sentence Transformers)  \n",
        "\n",
        "```python\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "vector_data = embeddings.embed_documents(chunks)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ **6. Instantiate the Vector Store**  \n",
        "Choose a **Vector Database**:  \n",
        "âœ… **Local**: FAISS, ChromaDB  \n",
        "âœ… **Cloud**: Pinecone, Weaviate, MongoDBAtlasVectorSearch  \n",
        "\n",
        "Example using **FAISS**:  \n",
        "```python\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ **7. Add Chunk Embeddings to the Vector Store**  \n",
        "Store **chunked embeddings** for fast retrieval:  \n",
        "\n",
        "```python\n",
        "vectorstore.add_documents(chunks)\n",
        "```\n",
        "\n",
        "Now, **queries** can retrieve the **most relevant** document chunks!\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ **8. Initialize the Chat Model**  \n",
        "Use **local** or **cloud-based** LLMs:  \n",
        "âœ… **Local:** Ollama, LM Studio  \n",
        "âœ… **Cloud:** OpenAI, AzureAI, AWS  \n",
        "\n",
        "```python\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ **9. Set Instructions for the Model**  \n",
        "Define **chatting templates** or **string prompts** to guide the model.  \n",
        "\n",
        "âœ… Example Prompt:  \n",
        "```\n",
        "Use the retrieved context to answer the user's query concisely.\n",
        "```\n",
        "\n",
        "```python\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"Answer the question based on context:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ **10. Create the Retrieval Chain**  \n",
        "**Chains** sequence multiple **LLM calls, tools, and preprocessing steps**.  \n",
        "Use **LCEL (LangChain Expression Language)** to build a chain.\n",
        "\n",
        "```python\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())  ### Example Chain (Now deprecated)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… **Final Architecture**  \n",
        "```\n",
        "1ï¸âƒ£ User Query  â  2ï¸âƒ£ Query Embedding  â  3ï¸âƒ£ Vector Search  \n",
        "  â  4ï¸âƒ£ Retrieve Top-K Chunks  â  5ï¸âƒ£ LLM Generation  â  6ï¸âƒ£ Final Answer  \n",
        "```\n",
        "\n",
        "ğŸ”¹ Now, your **RAG system** is ready to **retrieve & generate** answers efficiently! ğŸš€  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPTr2Ggarrkg"
      },
      "source": [
        "### STEP-1\n",
        "Gather documents\n",
        "We will be using text type documents and pdf type documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmXrPYMGrrki",
        "outputId": "53c83249-6cc9-4422-dcc5-b24597028991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/2.5 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/300.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain_community pypdf  # You can checkout complete list of document loaders available in langchain at https://python.langchain.com/docs/integrations/document_loaders/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBjmNtu7rrkl"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain (Install lanchain)\n",
        "\n",
        "from langchain.document_loaders import TextLoader # Simple TextLoader\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader  # One of the many PDF loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5AMwTKUrrkn",
        "outputId": "6b534bc7-a171-4fdf-b6c3-0a3f2d1be06d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (5.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install chardet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06ejGKLvrrkp"
      },
      "source": [
        "### STEP-2\n",
        "Load relevant documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD83p3m8rrkq",
        "outputId": "b443540c-592e-4971-a03e-96cd5a0ba6cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "137\n",
            "{'producer': 'calibre (0.7.50) [http://calibre-ebook.com]',\n",
            " 'creator': 'calibre (0.7.50) [http://calibre-ebook.com]',\n",
            " 'creationdate': '2022-09-20T04:19:35+00:00',\n",
            " 'author': 'Franklin W. Dixon',\n",
            " 'keywords': 'Hardy Boys (Fictitious characters), Detective and mystery '\n",
            "             'stories, Brothers, Teenage boy detectives, Mystery & Detective, '\n",
            "             'Juvenile Fiction, Mysteries & Detective Stories, General, '\n",
            "             \"Children's stories; American, Mystery fiction, Fiction, \"\n",
            "             'Detective and mystery stories; American, Mystery and detective '\n",
            "             'stories',\n",
            " 'moddate': '2022-09-20T04:19:36+00:00',\n",
            " 'title': 'The Tower Treasure',\n",
            " 'source': 'book1.pdf',\n",
            " 'total_pages': 137,\n",
            " 'page': 0,\n",
            " 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "loader = PyPDFLoader(\n",
        "    \"book1.pdf\",\n",
        "    mode=\"page\",                                            # You can use mode =\"simple\" as well for whole doc as single unit\n",
        ")\n",
        "docs = loader.load()\n",
        "print(len(docs))\n",
        "pprint.pp(docs[0].metadata)\n",
        "\n",
        "\n",
        "# loader = TextLoader('odyssey.txt',autodetect_encoding=True) # In case text document has unknown encodings use autodetect\n",
        "# docs = loader.load()\n",
        "# docs = loader.load()\n",
        "# print(len(docs))\n",
        "# pprint.pp(docs[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqqPftUMvyKs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbzZS1n5rrks"
      },
      "source": [
        "### STEP-3 & 4\n",
        "Splitting the text into chunks\n",
        "\n",
        "There are various methods to split the text in langchain\n",
        "\n",
        "1. Character splitter: Splits based on characters\n",
        "2. Sentence splitter: Splits based on sentences\n",
        "3. Token splitter: Splits based on tokens\n",
        "4. Recursive splitter: Mix of sentences, paragraphs (Most used)\n",
        "5. You can create your own as well!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR6D9tVyrrkt"
      },
      "outputs": [],
      "source": [
        "# import various splitters\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    SentenceTransformersTokenTextSplitter,\n",
        "    TextSplitter,\n",
        "    TokenTextSplitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P5qm4mErrkv",
        "outputId": "a4da4d9a-9a52-4250-c1d8-1c51f7507bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Using Recursive Character-based Splitting ---\n"
          ]
        }
      ],
      "source": [
        "# # Useful for consistent chunk sizes regardless of content structure.\n",
        "# print(\"\\n--- Using Character-based Splitting ---\")\n",
        "# char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "# char_docs = char_splitter.split_documents(docs)\n",
        "\n",
        "# # Splits text into chunks based on sentences, ensuring chunks end at sentence boundaries.\n",
        "# # Ideal for maintaining semantic coherence within chunks.\n",
        "# print(\"\\n--- Using Sentence-based Splitting ---\")\n",
        "# sent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\n",
        "# sent_docs = sent_splitter.split_documents(docs)\n",
        "\n",
        "# # Splits text into chunks based on tokens (words or subwords), using tokenizers like GPT-2.\n",
        "# # Useful for transformer models with strict token limits.\n",
        "# print(\"\\n--- Using Token-based Splitting ---\")\n",
        "# token_splitter = TokenTextSplitter(chunk_overlap=0, chunk_size=512)\n",
        "# token_docs = token_splitter.split_documents(docs)\n",
        "\n",
        "# Attempts to split text at natural boundaries (sentences, paragraphs) within character limit.\n",
        "# Balances between maintaining coherence and adhering to character limits.\n",
        "print(\"\\n--- Using Recursive Character-based Splitting ---\")\n",
        "rec_char_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512, chunk_overlap=0)\n",
        "rec_char_docs = rec_char_splitter.split_documents(docs)\n",
        "\n",
        "# Allows creating custom splitting logic based on specific requirements.\n",
        "# Useful for documents with unique structure that standard splitters can't handle.\n",
        "# print(\"\\n--- Using Custom Splitting ---\")\n",
        "\n",
        "\n",
        "# class CustomTextSplitter(TextSplitter):\n",
        "#     def split_text(self, text):\n",
        "#         # Custom logic for splitting text\n",
        "#         return text.split(\"\\n\\n\")  # Example: split by paragraphs\n",
        "\n",
        "\n",
        "# custom_splitter = CustomTextSplitter()\n",
        "# custom_docs = custom_splitter.split_documents(docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhgUphITrrkz"
      },
      "source": [
        "### STEP-5\n",
        "\n",
        "Create embeddings of the chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoLoHHs1yTUk",
        "outputId": "c2468fcd-2752-41d3-8bf0-5db1ecb55335"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-huggingface sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW-yLrUfyXVu"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "# embeddings = OllamaEmbeddings(\n",
        "#     model=\"nomic-embed-text\",                    # nomic-embed-text is a large context length text encoder that surpasses OpenAI text-embedding-ada-002 and text-embedding-3-small performance on short and long context tasks.\n",
        "# )\n",
        "\n",
        "# import getpass\n",
        "# import os\n",
        "\n",
        "# if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "#   os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
        "\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR3UsakBrrk1"
      },
      "source": [
        "### STEP-6\n",
        "\n",
        "Instantiate the vectorDB of your choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcRbSafcrrk1"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# vector_store = Chroma(embedding_function=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcRxnm9mrrk2"
      },
      "source": [
        "### STEP-7\n",
        "\n",
        "Add chunk embeddings to VectorDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpbYJXu7rrk4",
        "outputId": "51d7bf8e-47c8-4d56-91af-b344bcba24be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Creating vector store vector_store ---\n",
            "--- Finished creating vector store <langchain_chroma.vectorstores.Chroma object at 0x0000013FF49D8B60> ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "db_dir = os.path.join(current_dir, \"db3\")\n",
        "\n",
        "# Create function which stops from repititve addition of data into DB multiple times\n",
        "def create_vector_store(docs, store_name):\n",
        "    persistent_directory = os.path.join(db_dir, store_name)  # ChromaDB storage location\n",
        "    if not os.path.exists(persistent_directory):\n",
        "        print(f\"\\n--- Creating vector store {store_name} ---\")\n",
        "        store_name = Chroma.from_documents(\n",
        "            docs, embeddings, persist_directory=persistent_directory\n",
        "        )\n",
        "        print(f\"--- Finished creating vector store {store_name} ---\")\n",
        "    else:\n",
        "        print(\n",
        "            f\"Vector store {store_name} already exists. No need to initialize.\")\n",
        "\n",
        "create_vector_store(docs,'vector_store')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-WaPDXmrrk5",
        "outputId": "137eeb2b-f7ab-467e-d248-74c9e845ef41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Querying the Vector Store vector_store ---\n",
            "\n",
            "--- Relevant Documents for vector_store ---\n",
            "Document 1:\n",
            "Of these men, the Hardys took the reports on the ones who were thin and of\n",
            "medium height.\n",
            "Next came a check by telephone on the whereabouts of these people. All\n",
            "could be accounted for as working some distance from Bayport at the time\n",
            "of the thefts, with one exception.\n",
            "â€œIâ€™ll bet heâ€™s our man!â€ Frank exclaimed. â€œBut where is he now?â€\n",
            "\n",
            "Source: book1.pdf\n",
            "\n",
            "Document 2:\n",
            "Hardy. Hi, chums!â€ he said cheerily. â€œSorry to be late. My dad had a lot of\n",
            "phoning to do before he left. I was afraid if Iâ€™d tried to walk here, I\n",
            "wouldnâ€™t have arrived until tomorrow.â€\n",
            "At this point Mr. Hardy spoke up. â€œAs I said before, I think you boys should\n",
            "work in twos. There are only three of you to take care of half the territory.â€\n",
            "The detective suddenly grinned boyishly. â€œHow about me teaming up with\n",
            "one of you?â€\n",
            "Frank and Joe looked at their dad in delight. â€œYou mean it?â€ Frank cried\n",
            "out. â€œIâ€™ll choose you as my partner right now.â€\n",
            "â€œI have a further suggestion,â€ the detective said. â€œItâ€™s not going to take you\n",
            "fellows more than three hours to cover the area youâ€™ve laid out. And thereâ€™s\n",
            "an additional section I think you might look into.â€\n",
            "â€œWhatâ€™s that?â€ Joe inquired.\n",
            "â€œWillow Grove. Thatâ€™s a park area, but thereâ€™s also a lot of tangled\n",
            "woodland to one side of it. Good place to hide a stolen car.â€\n",
            "Mr. Hardy suggested that the boys meet for a picnic lunch at Willow Grove\n",
            "and later do some sleuthing in the vicinity. â€œThat is, provided you havenâ€™t\n",
            "found Chetâ€™s jalopy by that time.â€\n",
            "Mrs. Hardy spoke up. â€œIâ€™ll fix a nice lunch for all of you,â€ she offered.\n",
            "â€œThat sure would be swell,â€ Chet said hastily. â€œYou make grand picnic\n",
            "lunches, Mrs. Hardy.â€\n",
            "Frank and Joe liked the plan, and it was decided that the boys would have\n",
            "the picnic whether or not they had found the jalopy by one oâ€™clock. Mrs.\n",
            "Hardy said she would relay the news to the other boys when they phoned in.\n",
            "Chet and Joe set off on the Hardy boysâ€™ motorcycles, taking the twelve-to-\n",
            "three segment on the map. Then Mr. Hardy and Frank drove off for the\n",
            "three-to-six area.\n",
            "\n",
            "Source: book1.pdf\n",
            "\n",
            "Document 3:\n",
            "At the next crossing they found old Mike Hal-ley, the flagman there, busy at\n",
            "his job. His bright blue eyes searched their faces for a moment, then he\n",
            "amazed them by saying, â€œYouâ€™re Frank and Joe Hardy, sons of the famous\n",
            "detective Fenton Hardy.â€\n",
            "â€œYou know us?â€ Frank asked. â€œI must confess I donâ€™t recall having met you\n",
            "before.â€\n",
            "â€œAnd you ainâ€™t,â€ the man responded. â€œBut I make it a rule to memorize\n",
            "every face I see in the newspapers. Never know when thereâ€™s goinâ€™ to be an\n",
            "accident and I might be called on to identify some people.â€\n",
            "The boys gulped at this gruesome thought, then Frank asked Halley if he\n",
            "remembered a railroad man named Red Jackley.\n",
            "â€œI recollect a man named Jackley, but he wasnâ€™t never called Red when I\n",
            "knew him. I reckon heâ€™s the same fellow, though. You mean the one that I\n",
            "read went to jail?â€\n",
            "â€œThatâ€™s the man!â€\n",
            "â€œHe out of the pen yet?â€ Mike Halley questioned.\n",
            "â€œHe died,â€ Joe replied. â€œOur dad is working on a case that has some\n",
            "connection with Jackley and weâ€™re just trying to find out something about\n",
            "him.â€ â€œThen what you want to do,â€ said the flagman, â€œis go down to the\n",
            "Bayport and Coast Line Railroad. Thatâ€™s where Jackley used to work. He\n",
            "was around the station at Cherryville. That ainâ€™t so far from here.â€ He\n",
            "pointed in a northerly direction. â€œThanks a million,â€ said Frank. â€œYouâ€™ve\n",
            "helped us a lot.â€\n",
            "The brothers set off on their motorcycles for Cherryville. When they came\n",
            "to the small town, a policeman directed them to the railroad station, which\n",
            "was about a half mile out of town. The station stood in a depression below a\n",
            "new highway, and was reached by a curving road which ran parallel to the\n",
            "tracks for several hundred feet.\n",
            "\n",
            "Source: book1.pdf\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(id='416a5ab7-c443-4cbd-8e4c-71e78fc73e2d', metadata={'author': 'Franklin W. Dixon', 'creationdate': '2022-09-20T04:19:35+00:00', 'creator': 'calibre (0.7.50) [http://calibre-ebook.com]', 'keywords': \"Hardy Boys (Fictitious characters), Detective and mystery stories, Brothers, Teenage boy detectives, Mystery & Detective, Juvenile Fiction, Mysteries & Detective Stories, General, Children's stories; American, Mystery fiction, Fiction, Detective and mystery stories; American, Mystery and detective stories\", 'moddate': '2022-09-20T04:19:36+00:00', 'page': 71, 'page_label': '72', 'producer': 'calibre (0.7.50) [http://calibre-ebook.com]', 'source': 'book1.pdf', 'title': 'The Tower Treasure', 'total_pages': 137}, page_content='Of these men, the Hardys took the reports on the ones who were thin and of\\nmedium height.\\nNext came a check by telephone on the whereabouts of these people. All\\ncould be accounted for as working some distance from Bayport at the time\\nof the thefts, with one exception.\\nâ€œIâ€™ll bet heâ€™s our man!â€ Frank exclaimed. â€œBut where is he now?â€'),\n",
              " Document(id='5328223c-4c69-4355-a845-660392eff1bd', metadata={'author': 'Franklin W. Dixon', 'creationdate': '2022-09-20T04:19:35+00:00', 'creator': 'calibre (0.7.50) [http://calibre-ebook.com]', 'keywords': \"Hardy Boys (Fictitious characters), Detective and mystery stories, Brothers, Teenage boy detectives, Mystery & Detective, Juvenile Fiction, Mysteries & Detective Stories, General, Children's stories; American, Mystery fiction, Fiction, Detective and mystery stories; American, Mystery and detective stories\", 'moddate': '2022-09-20T04:19:36+00:00', 'page': 36, 'page_label': '37', 'producer': 'calibre (0.7.50) [http://calibre-ebook.com]', 'source': 'book1.pdf', 'title': 'The Tower Treasure', 'total_pages': 137}, page_content='Hardy. Hi, chums!â€ he said cheerily. â€œSorry to be late. My dad had a lot of\\nphoning to do before he left. I was afraid if Iâ€™d tried to walk here, I\\nwouldnâ€™t have arrived until tomorrow.â€\\nAt this point Mr. Hardy spoke up. â€œAs I said before, I think you boys should\\nwork in twos. There are only three of you to take care of half the territory.â€\\nThe detective suddenly grinned boyishly. â€œHow about me teaming up with\\none of you?â€\\nFrank and Joe looked at their dad in delight. â€œYou mean it?â€ Frank cried\\nout. â€œIâ€™ll choose you as my partner right now.â€\\nâ€œI have a further suggestion,â€ the detective said. â€œItâ€™s not going to take you\\nfellows more than three hours to cover the area youâ€™ve laid out. And thereâ€™s\\nan additional section I think you might look into.â€\\nâ€œWhatâ€™s that?â€ Joe inquired.\\nâ€œWillow Grove. Thatâ€™s a park area, but thereâ€™s also a lot of tangled\\nwoodland to one side of it. Good place to hide a stolen car.â€\\nMr. Hardy suggested that the boys meet for a picnic lunch at Willow Grove\\nand later do some sleuthing in the vicinity. â€œThat is, provided you havenâ€™t\\nfound Chetâ€™s jalopy by that time.â€\\nMrs. Hardy spoke up. â€œIâ€™ll fix a nice lunch for all of you,â€ she offered.\\nâ€œThat sure would be swell,â€ Chet said hastily. â€œYou make grand picnic\\nlunches, Mrs. Hardy.â€\\nFrank and Joe liked the plan, and it was decided that the boys would have\\nthe picnic whether or not they had found the jalopy by one oâ€™clock. Mrs.\\nHardy said she would relay the news to the other boys when they phoned in.\\nChet and Joe set off on the Hardy boysâ€™ motorcycles, taking the twelve-to-\\nthree segment on the map. Then Mr. Hardy and Frank drove off for the\\nthree-to-six area.'),\n",
              " Document(id='ac682334-1fde-4ae9-a605-af31dddfa294', metadata={'author': 'Franklin W. Dixon', 'creationdate': '2022-09-20T04:19:35+00:00', 'creator': 'calibre (0.7.50) [http://calibre-ebook.com]', 'keywords': \"Hardy Boys (Fictitious characters), Detective and mystery stories, Brothers, Teenage boy detectives, Mystery & Detective, Juvenile Fiction, Mysteries & Detective Stories, General, Children's stories; American, Mystery fiction, Fiction, Detective and mystery stories; American, Mystery and detective stories\", 'moddate': '2022-09-20T04:19:36+00:00', 'page': 119, 'page_label': '120', 'producer': 'calibre (0.7.50) [http://calibre-ebook.com]', 'source': 'book1.pdf', 'title': 'The Tower Treasure', 'total_pages': 137}, page_content='At the next crossing they found old Mike Hal-ley, the flagman there, busy at\\nhis job. His bright blue eyes searched their faces for a moment, then he\\namazed them by saying, â€œYouâ€™re Frank and Joe Hardy, sons of the famous\\ndetective Fenton Hardy.â€\\nâ€œYou know us?â€ Frank asked. â€œI must confess I donâ€™t recall having met you\\nbefore.â€\\nâ€œAnd you ainâ€™t,â€ the man responded. â€œBut I make it a rule to memorize\\nevery face I see in the newspapers. Never know when thereâ€™s goinâ€™ to be an\\naccident and I might be called on to identify some people.â€\\nThe boys gulped at this gruesome thought, then Frank asked Halley if he\\nremembered a railroad man named Red Jackley.\\nâ€œI recollect a man named Jackley, but he wasnâ€™t never called Red when I\\nknew him. I reckon heâ€™s the same fellow, though. You mean the one that I\\nread went to jail?â€\\nâ€œThatâ€™s the man!â€\\nâ€œHe out of the pen yet?â€ Mike Halley questioned.\\nâ€œHe died,â€ Joe replied. â€œOur dad is working on a case that has some\\nconnection with Jackley and weâ€™re just trying to find out something about\\nhim.â€ â€œThen what you want to do,â€ said the flagman, â€œis go down to the\\nBayport and Coast Line Railroad. Thatâ€™s where Jackley used to work. He\\nwas around the station at Cherryville. That ainâ€™t so far from here.â€ He\\npointed in a northerly direction. â€œThanks a million,â€ said Frank. â€œYouâ€™ve\\nhelped us a lot.â€\\nThe brothers set off on their motorcycles for Cherryville. When they came\\nto the small town, a policeman directed them to the railroad station, which\\nwas about a half mile out of town. The station stood in a depression below a\\nnew highway, and was reached by a curving road which ran parallel to the\\ntracks for several hundred feet.')]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def query_vector_store(store_name, query, embedding_function):\n",
        "    persistent_directory = os.path.join(db_dir, store_name)\n",
        "    if os.path.exists(persistent_directory):\n",
        "        print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
        "        db = Chroma(\n",
        "            persist_directory=persistent_directory,\n",
        "            embedding_function=embedding_function,\n",
        "        )\n",
        "        retriever = db.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": 3},\n",
        "        )\n",
        "        relevant_docs = retriever.invoke(query)\n",
        "        # Display the relevant results with metadata\n",
        "        print(f\"\\n--- Relevant Documents for {store_name} ---\")\n",
        "        for i, doc in enumerate(relevant_docs, 1):\n",
        "            print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "            if doc.metadata:\n",
        "                print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
        "        return relevant_docs\n",
        "    else:\n",
        "        print(f\"Vector store {store_name} does not exist.\")\n",
        "\n",
        "query = \"Who is frank hardy ?\"\n",
        "query_vector_store(\"vector_store\", query, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3dNYbEZrrk7"
      },
      "source": [
        "### Verification step\n",
        "\n",
        "You can manually check whether the data is correctly added to DB my manually querying into DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbd5KI2drrk8"
      },
      "outputs": [],
      "source": [
        "query = \"Who is frank hardy ?\"\n",
        "# Retrieve relevant documents based on the query\n",
        "\n",
        "relevant_docs = query_vector_store(\"vector_store\", query, embeddings)\n",
        "# print(relevant_docs)\n",
        "# Display the relevant results with metadata\n",
        "print(\"\\n--- Relevant Documents ---\")\n",
        "for i, doc in enumerate(relevant_docs, 1):\n",
        "    # print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "    if doc.metadata:\n",
        "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS7q0S8frrk9"
      },
      "source": [
        "### STEP-8\n",
        "\n",
        "Initialize the chat model of your choice OpenAI or Ollama or HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrcbDKRTrrk9",
        "outputId": "a6cb3e62-9e1d-4290-c715-a218be4b9d32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='**Fibonacci Series in Python**\\n=====================================\\n\\nHere is a simple Python function to generate the Fibonacci series up to the 6th term:\\n\\n```python\\ndef fibonacci(n):\\n    \"\"\"\\n    Generate the Fibonacci series up to the nth term.\\n\\n    Args:\\n        n (int): The number of terms in the series.\\n\\n    Returns:\\n        list: A list of integers representing the Fibonacci series.\\n    \"\"\"\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    elif n == 2:\\n        return [0, 1]\\n\\n    fib_series = [0, 1]\\n    while len(fib_series) < n:\\n        fib_series.append(fib_series[-1] + fib_series[-2])\\n\\n    return fib_series\\n\\n# Print the Fibonacci series up to the 6th term\\nprint(\"Fibonacci Series:\")\\nfor i in range(1, 7):\\n    print(f\"F({i}) = {fibonacci(i)[i-1]}\")\\n```\\n\\n**Output:**\\n```\\nFibonacci Series:\\nF(1) = 0\\nF(2) = 1\\nF(3) = 1\\nF(4) = 2\\nF(5) = 3\\nF(6) = 5\\n```\\n\\nThis code defines a function `fibonacci(n)` that generates the Fibonacci series up to the nth term. It uses a simple iterative approach, starting with the base cases for n=1 and n=2, and then appending each subsequent term to the list using the recursive formula `F(n) = F(n-1) + F(n-2)`. The function returns a list of integers representing the Fibonacci series.\\n\\nThe main part of the code prints the Fibonacci series up to the 6th term by calling the `fibonacci(i)` function for each term from 1 to 6, and then printing the corresponding value in the series.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-02-15T04:59:56.1915126Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2487472100, 'load_duration': 10407200, 'prompt_eval_count': 54, 'prompt_eval_duration': 103000000, 'eval_count': 403, 'eval_duration': 2145000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-5365b076-4839-4642-a39d-c52f50d29132-0', usage_metadata={'input_tokens': 54, 'output_tokens': 403, 'total_tokens': 457})"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# messages = [\n",
        "#     SystemMessage(content=\"Solve the following math problems\"),\n",
        "#     HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "# ]\n",
        "\n",
        "# model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
        "\n",
        "# model.invoke(\"Hello, world!\")\n",
        "# # Create a ChatOpenAI model\n",
        "# model = init_chat_modelodel=\"gpt-4o\")\n",
        "\n",
        "# # Invoke the model with messages\n",
        "# result = model.invoke(messages)\n",
        "# print(f\"Answer from OpenAI: {result.content}\")\n",
        "\n",
        "\n",
        "# # ---- Anthropic Chat Model Example ----\n",
        "\n",
        "# # Create a Anthropic model\n",
        "# # Anthropic models: https://docs.anthropic.com/en/docs/models-overview\n",
        "# model = init_chat_modeldel=(\"claude-3-opus-20240229\")\n",
        "\n",
        "# result = model.invoke(messages)\n",
        "# print(f\"Answer from Anthropic: {result.content}\")\n",
        "\n",
        "\n",
        "# # ---- Google Chat Model Example ----\n",
        "\n",
        "# # https://console.cloud.google.com/gen-app-builder/engines\n",
        "# # https://ai.google.dev/gemini-api/docs/models/gemini\n",
        "# model = init_chat_modelmodel=(\"gemini-1.5-flash\")\n",
        "\n",
        "# result = model.invoke(messages)\n",
        "# print(f\"Answer from Google: {result.content}\")\n",
        "%pip install --upgrade --quiet  langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.2\",\n",
        "    temperature=0,\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that writes professional code. Write good quality code.\",\n",
        "    ),\n",
        "    (\"human\", \"write python code to print fibonaci series til 6th \"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-iS0sUHrrk-"
      },
      "source": [
        "### STEP-9\n",
        "\n",
        "Set instructions for model\n",
        "1. Set prompt for the model\n",
        "2. Combine the retrieval output and the user query (Chaining)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKWcM8j3rrk_",
        "outputId": "f581487b-5045-480d-b493-ba82b241d3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided documents, I can tell you that Frank Hardy is one of the main characters mentioned. He is a young man who works with his brother Joe and their father, Fenton Hardy, to solve mysteries and crimes.\n"
          ]
        }
      ],
      "source": [
        "# from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessage\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_ollama import ChatOllama\n",
        "query = \"Who is frank hardy?\"\n",
        "combined_input = (\n",
        "    \"Here are some documents that might help answer the question: \"\n",
        "    + query\n",
        "    + \"\\n\\nRelevant Documents:\\n\"\n",
        "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    + \"\\n\\nPlease provide an answer based only on the provided documents. If the answer is not found in the documents, respond with 'I'm not sure'.\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=combined_input),\n",
        "]\n",
        "\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.2\",\n",
        "    temperature=0,\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "\n",
        "# Invoke the model with the combined input\n",
        "result = llm.invoke(messages)\n",
        "# Create the ChatPromptTemplate with messages\n",
        "\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB42L2wyrrk_"
      },
      "source": [
        "### STEP-10\n",
        "\n",
        "We will invoke use invoke function with retrieved_doc as context and user query as question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyzEmlEcrrlA",
        "outputId": "9fc376ec-410f-43ab-ea05-37f9f73a14f8"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'RunnableSequence' object has no attribute 'format_messages'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m message1 \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m(context \u001b[38;5;241m=\u001b[39m {relevant_docs},user_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the name of the book ?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m chain\u001b[38;5;241m.\u001b[39minvoke(message1)\n",
            "File \u001b[1;32mc:\\GenAI 2025\\rag\\Lib\\site-packages\\pydantic\\main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'RunnableSequence' object has no attribute 'format_messages'"
          ]
        }
      ],
      "source": [
        "message1 = chain.format_messages(context = {relevant_docs},user_input=\"What is the name of the book ?\")\n",
        "chain.invoke(message1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgxhf7o0rrlB"
      },
      "source": [
        "### Using community made end-to-end chains combined with retrievars\n",
        "We can optionally use prebuilt chains as well to avoid the fuss like RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfzfjdu1rrlC",
        "outputId": "649d3bcd-c970-4a2e-8d5c-75a211225ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Querying the Vector Store  ---\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "persistent_directory = os.path.join(db_dir, 'vector_store')\n",
        "\n",
        "\n",
        "if os.path.exists(persistent_directory):\n",
        "    print(f\"\\n--- Querying the Vector Store  ---\")\n",
        "    db = Chroma(\n",
        "        persist_directory=persistent_directory,\n",
        "        embedding_function=embeddings,\n",
        "    )\n",
        "    retriever = db.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 3},\n",
        "    )\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "    You are a helpful AI assistant that answers questions based on the provided PDF document.\n",
        "    Use only the context provided to answer the question. If you don't know the answer or\n",
        "    can't find it in the context, say so.\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer: Let me help you with that based on the PDF content.\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # 6. Create and return the QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,  # in this we have added the retriever in the chain itself instead of querying it manually first\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": PROMPT}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voEz2Qe1rrlD",
        "outputId": "514cf2f5-8868-453a-fcb6-d6045a98541d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frank Hardy is one of the main characters in the story. He is the son of Fenton Hardy, a famous detective, and is also a young detective himself who works with his brother Joe to solve cases.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "system_prompt = (\n",
        "    \"Use the given context to answer the question. \"\n",
        "    \"If you don't know the answer, say you don't know. \"\n",
        "    \"Use three sentence maximum and keep the answer concise. \"\n",
        "    \"Context: {context}\"\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "result = chain.invoke({\"input\": query})\n",
        "\n",
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
      "state": {},
      "version_major": 2,
      "version_minor": 0

      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
